{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLMMKPoB3xRboDxYGKbv7Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_05/blob/main/LM_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Language Model (CLM) fine-tuning\n",
        "\n",
        "This notebook executes the fine-tuning of **facebook/opt-125m** model, over the mc4 pt dataset samples prepared by the `LM_training_dataset_preparation.ipynb` notebook."
      ],
      "metadata": {
        "id": "jm10mcBmMyUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "IyoODiJf7UDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f89210-8745-4620-fa48-7f15dfe15560"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_05\"\n",
        "\n",
        "API_KEYS_FILE=\"/content/drive/MyDrive/unicamp/ia368v_dd/api_keys_20230324.json\"\n",
        "\n",
        "TRAIN_OUTPUT_FOLDER=\"./trained_model\"\n",
        "\n",
        "NORMALIZED_DATA_BLOCKS_PARTIAL_FILENAME=\"normalized_samples_block_??.pkl\""
      ],
      "metadata": {
        "id": "gB0nBfaiLYJC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import json"
      ],
      "metadata": {
        "id": "J1H5lQXWbFF5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "mFNsI1WXLtsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41bef1ec-58b0-4fe3-c207-f9df5825707f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(API_KEYS_FILE) as inputFile:\n",
        "#     api_keys = json.load(inputFile)\n",
        "\n",
        "# os.environ[\"COMET_API_KEY\"] = api_keys['comet_ml']\n",
        "# os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "# os.environ['COMET_MODE'] = \"ONLINE\""
      ],
      "metadata": {
        "id": "Exz2AKVDbGmK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from comet_ml import Experiment\n",
        "\n",
        "from transformers import (AutoTokenizer, \n",
        "                          AutoModelForCausalLM, \n",
        "                          Trainer, \n",
        "                          TrainingArguments, \n",
        "                          TrainerCallback, \n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup)\n",
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union"
      ],
      "metadata": {
        "id": "2iEim9qqLRab"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DcXd955oJhRE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "Iet6vPCwJhRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c4c77d1-b61e-42b0-a1a6-aacef8cf0ef1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "44pKblR9uKJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892d5ace-d9ca-4ddb-cd18-76d1ab5954b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 29 23:15:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME=\"facebook/opt-125m\""
      ],
      "metadata": {
        "id": "7k9UgWy7Wf77"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link to Comet ML reporting"
      ],
      "metadata": {
        "id": "c2pyJsxgHxCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment(api_key=api_keys['comet_ml'], \n",
        "#            project_name=\"causal-language-model-fine-tuning\",\n",
        "#            workspace=\"eduseiti\")"
      ],
      "metadata": {
        "id": "Rxf3Y8TjH0iB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the list of normalized-tokenized samples data blocked_samples\n",
        "\n",
        "The mc4 pt dataset sample has already been tokenized and size-normalized to 512, which is the model input size.\n",
        "\n",
        "Each data block contains a list of prepared samples, each of which can be directly fed to the model:\n",
        "\n",
        "```\n",
        "    {'input_ids': <list-of-512-tokens>,\n",
        "     'attention_masks': <list-of-512-attention-masks>}\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GLsQX2lUwJtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_blocks = glob.glob(NORMALIZED_DATA_BLOCKS_PARTIAL_FILENAME)"
      ],
      "metadata": {
        "id": "eMIwdJE6wJyU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_blocks"
      ],
      "metadata": {
        "id": "_QGhlPPRwJ2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae5d329-19b5-4e8f-83ca-539a204a59d0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['normalized_samples_block_00.pkl',\n",
              " 'normalized_samples_block_01.pkl',\n",
              " 'normalized_samples_block_02.pkl',\n",
              " 'normalized_samples_block_03.pkl',\n",
              " 'normalized_samples_block_04.pkl',\n",
              " 'normalized_samples_block_05.pkl',\n",
              " 'normalized_samples_block_06.pkl',\n",
              " 'normalized_samples_block_07.pkl',\n",
              " 'normalized_samples_block_08.pkl',\n",
              " 'normalized_samples_block_09.pkl',\n",
              " 'normalized_samples_block_10.pkl',\n",
              " 'normalized_samples_block_11.pkl',\n",
              " 'normalized_samples_block_12.pkl',\n",
              " 'normalized_samples_block_13.pkl',\n",
              " 'normalized_samples_block_14.pkl',\n",
              " 'normalized_samples_block_15.pkl',\n",
              " 'normalized_samples_block_16.pkl',\n",
              " 'normalized_samples_block_17.pkl',\n",
              " 'normalized_samples_block_18.pkl',\n",
              " 'normalized_samples_block_19.pkl',\n",
              " 'normalized_samples_block_20.pkl',\n",
              " 'normalized_samples_block_21.pkl',\n",
              " 'normalized_samples_block_22.pkl',\n",
              " 'normalized_samples_block_23.pkl',\n",
              " 'normalized_samples_block_24.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the dataset class"
      ],
      "metadata": {
        "id": "SkHAtH-wMNV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples_blocks_filenames, fixed_data_block_index=None, sampling_size=None):\n",
        "        self.samples_blocks_filenames = samples_blocks_filenames\n",
        "\n",
        "        if fixed_data_block_index is not None:\n",
        "            self.current_file_index = fixed_data_block_index\n",
        "            self.change_file_index = False\n",
        "        else:\n",
        "            self.current_file_index = 0\n",
        "            self.change_file_index = True\n",
        "\n",
        "        with open(samples_blocks_filenames[self.current_file_index], \"rb\") as inputFile:\n",
        "            self.db = pickle.load(inputFile)\n",
        "\n",
        "        print(\"Dataset loading samples block {}; change_file_index={}...\".format(self.current_file_index, self.change_file_index))\n",
        "\n",
        "        self.sampling_size = sampling_size\n",
        "    \n",
        "        if self.sampling_size is not None:\n",
        "            self.dataset_size = self.sampling_size\n",
        "            self.sample_data()\n",
        "        else:\n",
        "            self.dataset_size = len(self.db)\n",
        "\n",
        "\n",
        "\n",
        "    def sample_data(self):\n",
        "        self.selected_samples = np.random.choice(list(range(len(self.db))), self.sampling_size, replace=False)\n",
        "        self.sampled_db = [self.db[i] for i in self.selected_samples]\n",
        "\n",
        "        print(\"Updating the sampled dataset itens; sample DB size: {}\".format(len(self.sampled_db)))\n",
        "\n",
        "\n",
        "\n",
        "    def update_dataset(self):\n",
        "        if self.change_file_index:\n",
        "            self.current_file_index = (self.current_file_index + 1) % len(self.samples_blocks_filenames)\n",
        "\n",
        "            with open(self.samples_blocks_filenames[self.current_file_index], \"rb\") as inputFile:\n",
        "                self.db = pickle.load(inputFile)\n",
        "\n",
        "            print(\"Updating dataset loading samples block {}; change_file_index={}...\".format(self.current_file_index, self.change_file_index))\n",
        "\n",
        "        if self.sampling_size is not None:\n",
        "            self.sample_data()\n",
        "        else:\n",
        "            self.dataset_size = len(self.db)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.sampling_size is not None:\n",
        "            return {'input_ids': self.sampled_db[idx]['input_ids'],\n",
        "                    'attention_mask': self.sampled_db[idx]['attention_mask'],\n",
        "                    'labels': self.sampled_db[idx]['input_ids'].copy()}\n",
        "        else:\n",
        "            return {'input_ids': self.db[idx]['input_ids'],\n",
        "                    'attention_mask': self.db[idx]['attention_mask'],\n",
        "                    'labels': self.db[idx]['input_ids'].copy()}"
      ],
      "metadata": {
        "id": "pk90h5aDx2q2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a callback to update the datasets and save a checkpoint of the best epoch yet."
      ],
      "metadata": {
        "id": "8sJYGLVq3sJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainerCallback(TrainerCallback):\n",
        "\n",
        "    def __init__(self, best_validation_yet=99999, model=None, train_dataset=None, eval_dataset=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.best_validation_loss = best_validation_yet\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.eval_dataset = eval_dataset\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, train_dataloader=None, eval_dataloader=None, **kwargs):\n",
        "        self.train_dataset.update_dataset()\n",
        "        self.eval_dataset.update_dataset()\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
        "        # print(metrics.keys())\n",
        "\n",
        "        try:\n",
        "            perplexity = np.exp(metrics[\"eval_loss\"])\n",
        "        except OverflowError:\n",
        "            perplexity = float(\"inf\")\n",
        "\n",
        "        metrics['perplexity'] = perplexity\n",
        "\n",
        "        # print(\"perplexity={}\".format(metrics['perplexity']))\n",
        "\n",
        "        if metrics['eval_loss'] < self.best_validation_loss:\n",
        "            self.model.save_pretrained(os.path.join(TRAIN_OUTPUT_FOLDER, \n",
        "                                                    \"checkpoint-{}-{:.4f}\".format(state.global_step,\n",
        "                                                                                  metrics['eval_loss'])))\n",
        "            self.best_validation_loss = metrics['eval_loss']"
      ],
      "metadata": {
        "id": "4zE1D83l2vYN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data"
      ],
      "metadata": {
        "id": "WpuFpHXyN-cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(data_blocks[6:-1])#, sampling_size=5000)"
      ],
      "metadata": {
        "id": "ZJc4Ak_H5tw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66ac484-2b1d-459a-f44d-bd55e7896332"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 0; change_file_index=True...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = Dataset(data_blocks, len(data_blocks) - 1, sampling_size=3000)"
      ],
      "metadata": {
        "id": "Dp8cjYTZ-VJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251798de-be82-4e7f-f741-4116da6b4c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 24; change_file_index=False...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model"
      ],
      "metadata": {
        "id": "Awzi5Ads5J6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"trained_model/checkpoint-after-05-complete-data-blocks\").to(device)\n",
        "print('Parameters', model.num_parameters())"
      ],
      "metadata": {
        "id": "g1QZgeFy58CM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432d76c5-a826-4be1-9766-12676fbe2300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters 125239296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the trainer"
      ],
      "metadata": {
        "id": "PrOgu5k96LTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=12\n",
        "epochs=19"
      ],
      "metadata": {
        "id": "kT5rUueI658D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                    num_train_epochs=epochs,\n",
        "                                    per_device_train_batch_size=batch_size,\n",
        "                                    per_device_eval_batch_size=batch_size,\n",
        "                                    evaluation_strategy='epoch',\n",
        "                                    save_strategy='steps',\n",
        "                                    save_steps=10000,\n",
        "                                    logging_strategy='steps',\n",
        "                                    logging_steps=10,\n",
        "                                    save_total_limit=10,\n",
        "                                    # report_to='comet_ml',\n",
        "                                    dataloader_num_workers=2,\n",
        "                                    dataloader_pin_memory=False,\n",
        "                                    fp16=True)"
      ],
      "metadata": {
        "id": "99L0Mq_p7s4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = CustomTrainerCallback(best_validation_yet=1.946620, \n",
        "                                         model=model, \n",
        "                                         train_dataset=train_dataset, \n",
        "                                         eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "id": "phSscTZI3hO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = epochs * len(train_dataset)\n",
        "\n",
        "optimzer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=1e-3)\n",
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimzer, \n",
        "                                                               num_training_steps * 0.1, \n",
        "                                                               num_training_steps, \n",
        "                                                               num_cycles=80)"
      ],
      "metadata": {
        "id": "8y9rWOcRbZoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  args=training_params,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=eval_dataset,\n",
        "                  callbacks=[trainer_callback],\n",
        "                  optimizers=(optimzer, scheduler)\n",
        "                 )"
      ],
      "metadata": {
        "id": "77Sgz4OB8nMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTFtBSwwgKdu",
        "outputId": "88151e17-2186-4b00-ad3f-81f5e3510b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='53870' max='60249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [53870/60249 9:49:12 < 1:09:46, 1.52 it/s, Epoch 17.05/19]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.054200</td>\n",
              "      <td>1.975033</td>\n",
              "      <td>7.206854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.001900</td>\n",
              "      <td>1.991585</td>\n",
              "      <td>7.327138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.139300</td>\n",
              "      <td>1.990324</td>\n",
              "      <td>7.317907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.996700</td>\n",
              "      <td>2.006491</td>\n",
              "      <td>7.437178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.036400</td>\n",
              "      <td>1.980272</td>\n",
              "      <td>7.244715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.950200</td>\n",
              "      <td>1.985172</td>\n",
              "      <td>7.280299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.045600</td>\n",
              "      <td>1.991627</td>\n",
              "      <td>7.327446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.970800</td>\n",
              "      <td>1.990485</td>\n",
              "      <td>7.319085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.969900</td>\n",
              "      <td>1.992419</td>\n",
              "      <td>7.333249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.978700</td>\n",
              "      <td>1.983041</td>\n",
              "      <td>7.264799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.978000</td>\n",
              "      <td>1.968126</td>\n",
              "      <td>7.157253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.021700</td>\n",
              "      <td>1.992287</td>\n",
              "      <td>7.332285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.924000</td>\n",
              "      <td>1.977049</td>\n",
              "      <td>7.221398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.013300</td>\n",
              "      <td>1.983407</td>\n",
              "      <td>7.267465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.016300</td>\n",
              "      <td>1.984537</td>\n",
              "      <td>7.275675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.952600</td>\n",
              "      <td>1.981306</td>\n",
              "      <td>7.252210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.013800</td>\n",
              "      <td>1.977582</td>\n",
              "      <td>7.225251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='54801' max='60249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [54801/60249 9:59:07 < 59:33, 1.52 it/s, Epoch 17.34/19]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.054200</td>\n",
              "      <td>1.975033</td>\n",
              "      <td>7.206854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.001900</td>\n",
              "      <td>1.991585</td>\n",
              "      <td>7.327138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.139300</td>\n",
              "      <td>1.990324</td>\n",
              "      <td>7.317907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.996700</td>\n",
              "      <td>2.006491</td>\n",
              "      <td>7.437178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.036400</td>\n",
              "      <td>1.980272</td>\n",
              "      <td>7.244715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.950200</td>\n",
              "      <td>1.985172</td>\n",
              "      <td>7.280299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.045600</td>\n",
              "      <td>1.991627</td>\n",
              "      <td>7.327446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.970800</td>\n",
              "      <td>1.990485</td>\n",
              "      <td>7.319085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.969900</td>\n",
              "      <td>1.992419</td>\n",
              "      <td>7.333249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.978700</td>\n",
              "      <td>1.983041</td>\n",
              "      <td>7.264799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.978000</td>\n",
              "      <td>1.968126</td>\n",
              "      <td>7.157253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.021700</td>\n",
              "      <td>1.992287</td>\n",
              "      <td>7.332285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.924000</td>\n",
              "      <td>1.977049</td>\n",
              "      <td>7.221398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.013300</td>\n",
              "      <td>1.983407</td>\n",
              "      <td>7.267465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.016300</td>\n",
              "      <td>1.984537</td>\n",
              "      <td>7.275675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.952600</td>\n",
              "      <td>1.981306</td>\n",
              "      <td>7.252210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.013800</td>\n",
              "      <td>1.977582</td>\n",
              "      <td>7.225251</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-24-a6eca412ee9e>\", line 1, in <module>\n",
            "    train_result = trainer.train()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1633, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 1979, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 2224, in _maybe_log_save_evaluate\n",
            "    self.log(logs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 2558, in log\n",
            "    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer_callback.py\", line 390, in on_log\n",
            "    return self.call_event(\"on_log\", args, state, control, logs=logs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer_callback.py\", line 397, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/integrations.py\", line 644, in on_log\n",
            "    self.tb_writer.flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/tensorboard/writer.py\", line 1200, in flush\n",
            "    writer.flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/tensorboard/writer.py\", line 150, in flush\n",
            "    self.event_writer.flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 121, in flush\n",
            "    self._async_writer.flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 177, in flush\n",
            "    self._writer.flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/summary/writer/record_writer.py\", line 43, in flush\n",
            "    self._writer.flush()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/lib/io/file_io.py\", line 221, in flush\n",
            "    self._writable_file.flush()\n",
            "tensorflow.python.framework.errors_impl.FailedPreconditionError: trained_model/runs/Mar28_20-45-22_5838f6061a91/events.out.tfevents.1680036322.5838f6061a91.750.0; Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'FailedPreconditionError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1543, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1501, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 738, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 722, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "  File \"/usr/lib/python3.9/posixpath.py\", line 380, in abspath\n",
            "    cwd = os.getcwd()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0G-3UqCFfVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just evaluate against the entire evaluation dataset"
      ],
      "metadata": {
        "id": "XwlW8R4UoF_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = Dataset(data_blocks, len(data_blocks) - 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152b505f-3ca1-4b13-f69a-5ef2acb709c1",
        "id": "pKDlJZ0woF_3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 24; change_file_index=False...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(eval_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfSSY_8_qivm",
        "outputId": "c88fdf97-dcd4-44f9-da18-f08a5a9d9fc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36869"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"trained_model/checkpoint-50000\").to(device)\n",
        "print('Parameters', model.num_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40ba06ef-f299-486d-f255-f634316eb727",
        "id": "0k3f_tI3nJjs"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters 125239296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the trainer"
      ],
      "metadata": {
        "id": "YoJvuv4bnJjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=4\n",
        "epochs=1"
      ],
      "metadata": {
        "id": "F4-hIZM_nJju"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                    num_train_epochs=epochs,\n",
        "                                    per_device_train_batch_size=batch_size,\n",
        "                                    per_device_eval_batch_size=batch_size,\n",
        "                                    evaluation_strategy='epoch',\n",
        "                                    save_strategy='steps',\n",
        "                                    save_steps=10000,\n",
        "                                    logging_strategy='steps',\n",
        "                                    logging_steps=10,\n",
        "                                    save_total_limit=10,\n",
        "                                    # report_to='comet_ml',\n",
        "                                    dataloader_num_workers=2,\n",
        "                                    dataloader_pin_memory=False,\n",
        "                                    fp16=True)"
      ],
      "metadata": {
        "id": "xhWqIRvJnJju"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = CustomTrainerCallback(best_validation_yet=1.94, \n",
        "                                         model=model, \n",
        "                                         train_dataset=train_dataset, \n",
        "                                         eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "id": "PYpEQaPemX_R"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = epochs * len(train_dataset)\n",
        "\n",
        "optimzer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimzer, \n",
        "                                                               num_training_steps * 0.1, \n",
        "                                                               num_training_steps, \n",
        "                                                               num_cycles=80)"
      ],
      "metadata": {
        "id": "1h6rLaHOmX_S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  args=training_params,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=eval_dataset,\n",
        "                  callbacks=[trainer_callback],\n",
        "                  optimizers=(optimzer, scheduler)\n",
        "                 )"
      ],
      "metadata": {
        "id": "bV6__ucynwYo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result = trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "jUKSb9OImZXe",
        "outputId": "e820ac35-04f2-473e-d699-26f94a38190f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9218' max='9218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9218/9218 10:38]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_result"
      ],
      "metadata": {
        "id": "YxZTGBlX8fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eedffe91-d63b-45bc-9947-5d53a950d0b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 1.9811569452285767,\n",
              " 'eval_runtime': 641.3556,\n",
              " 'eval_samples_per_second': 57.486,\n",
              " 'eval_steps_per_second': 14.373,\n",
              " 'perplexity': 7.251127291255752}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jgh00DUVuVBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}