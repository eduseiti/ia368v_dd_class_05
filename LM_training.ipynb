{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPYeMHGL08H4wrGTKWZfIsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_05/blob/main/LM_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Language Model (CLM) fine-tuning\n",
        "\n",
        "This notebook executes the fine-tuning of **facebook/opt-125m** model, over the mc4 pt dataset samples prepared by the `LM_training_dataset_preparation.ipynb` notebook."
      ],
      "metadata": {
        "id": "jm10mcBmMyUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "IyoODiJf7UDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_05\"\n",
        "\n",
        "API_KEYS_FILE=\"/content/drive/MyDrive/unicamp/ia368v_dd/api_keys_20230324.json\"\n",
        "\n",
        "TRAIN_OUTPUT_FOLDER=\"./trained_model\"\n",
        "\n",
        "NORMALIZED_DATA_BLOCKS_PARTIAL_FILENAME=\"normalized_samples_block_*\""
      ],
      "metadata": {
        "id": "gB0nBfaiLYJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import json"
      ],
      "metadata": {
        "id": "J1H5lQXWbFF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "mFNsI1WXLtsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ac24b3-fd8f-45a3-cca5-62b98e7366f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(API_KEYS_FILE) as inputFile:\n",
        "#     api_keys = json.load(inputFile)\n",
        "\n",
        "# os.environ[\"COMET_API_KEY\"] = api_keys['comet_ml']\n",
        "# os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "# os.environ['COMET_MODE'] = \"ONLINE\""
      ],
      "metadata": {
        "id": "Exz2AKVDbGmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from comet_ml import Experiment\n",
        "\n",
        "from transformers import (AutoTokenizer, \n",
        "                          AutoModelForCausalLM, \n",
        "                          Trainer, \n",
        "                          TrainingArguments, \n",
        "                          TrainerCallback, \n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup)\n",
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union"
      ],
      "metadata": {
        "id": "2iEim9qqLRab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DcXd955oJhRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "Iet6vPCwJhRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20a1641-2c03-4173-ec79-79e87c79c52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "44pKblR9uKJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c68369a-4d7b-4fa7-b17f-80be57cf65bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 27 23:38:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    12W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME=\"facebook/opt-125m\""
      ],
      "metadata": {
        "id": "7k9UgWy7Wf77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link to Comet ML reporting"
      ],
      "metadata": {
        "id": "c2pyJsxgHxCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment(api_key=api_keys['comet_ml'], \n",
        "#            project_name=\"causal-language-model-fine-tuning\",\n",
        "#            workspace=\"eduseiti\")"
      ],
      "metadata": {
        "id": "Rxf3Y8TjH0iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the list of normalized-tokenized samples data blocked_samples\n",
        "\n",
        "The mc4 pt dataset sample has already been tokenized and size-normalized to 512, which is the model input size.\n",
        "\n",
        "Each data block contains a list of prepared samples, each of which can be directly fed to the model:\n",
        "\n",
        "```\n",
        "    {'input_ids': <list-of-512-tokens>,\n",
        "     'attention_masks': <list-of-512-attention-masks>}\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GLsQX2lUwJtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_blocks = glob.glob(NORMALIZED_DATA_BLOCKS_PARTIAL_FILENAME)"
      ],
      "metadata": {
        "id": "eMIwdJE6wJyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_blocks"
      ],
      "metadata": {
        "id": "_QGhlPPRwJ2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcf4adb-f838-436d-ee7f-bd49597591ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['normalized_samples_block_00.pkl',\n",
              " 'normalized_samples_block_01.pkl',\n",
              " 'normalized_samples_block_02.pkl',\n",
              " 'normalized_samples_block_03.pkl',\n",
              " 'normalized_samples_block_04.pkl',\n",
              " 'normalized_samples_block_05.pkl',\n",
              " 'normalized_samples_block_06.pkl',\n",
              " 'normalized_samples_block_07.pkl',\n",
              " 'normalized_samples_block_08.pkl',\n",
              " 'normalized_samples_block_09.pkl',\n",
              " 'normalized_samples_block_10.pkl',\n",
              " 'normalized_samples_block_11.pkl',\n",
              " 'normalized_samples_block_12.pkl',\n",
              " 'normalized_samples_block_13.pkl',\n",
              " 'normalized_samples_block_14.pkl',\n",
              " 'normalized_samples_block_15.pkl',\n",
              " 'normalized_samples_block_16.pkl',\n",
              " 'normalized_samples_block_17.pkl',\n",
              " 'normalized_samples_block_18.pkl',\n",
              " 'normalized_samples_block_19.pkl',\n",
              " 'normalized_samples_block_20.pkl',\n",
              " 'normalized_samples_block_21.pkl',\n",
              " 'normalized_samples_block_22.pkl',\n",
              " 'normalized_samples_block_23.pkl',\n",
              " 'normalized_samples_block_24.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the dataset class"
      ],
      "metadata": {
        "id": "SkHAtH-wMNV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples_blocks_filenames, fixed_data_block_index=None, sampling_size=None):\n",
        "        self.samples_blocks_filenames = samples_blocks_filenames\n",
        "\n",
        "        if fixed_data_block_index is not None:\n",
        "            self.current_file_index = fixed_data_block_index\n",
        "            self.change_file_index = False\n",
        "        else:\n",
        "            self.current_file_index = 0\n",
        "            self.change_file_index = True\n",
        "\n",
        "        with open(samples_blocks_filenames[self.current_file_index], \"rb\") as inputFile:\n",
        "            self.db = pickle.load(inputFile)\n",
        "\n",
        "        print(\"Dataset loading samples block {}; change_file_index={}...\".format(self.current_file_index, self.change_file_index))\n",
        "\n",
        "        self.sampling_size = sampling_size\n",
        "    \n",
        "        if self.sampling_size is not None:\n",
        "            self.dataset_size = self.sampling_size\n",
        "            self.sample_data()\n",
        "        else:\n",
        "            self.dataset_size = len(self.db)\n",
        "\n",
        "\n",
        "\n",
        "    def sample_data(self):\n",
        "        self.selected_samples = np.random.choice(list(range(len(self.db))), self.sampling_size, replace=False)\n",
        "        self.sampled_db = [self.db[i] for i in self.selected_samples]\n",
        "\n",
        "        print(\"Updating the sampled dataset itens; sample DB size: {}\".format(len(self.sampled_db)))\n",
        "\n",
        "\n",
        "\n",
        "    def update_dataset(self):\n",
        "        if self.change_file_index:\n",
        "            self.current_file_index = (self.current_file_index + 1) % len(self.samples_blocks_filenames)\n",
        "\n",
        "            with open(self.samples_blocks_filenames[self.current_file_index], \"rb\") as inputFile:\n",
        "                self.db = pickle.load(inputFile)\n",
        "\n",
        "            print(\"Updating dataset loading samples block {}; change_file_index={}...\".format(self.current_file_index, self.change_file_index))\n",
        "\n",
        "        if self.sampling_size is not None:\n",
        "            self.sample_data()\n",
        "        else:\n",
        "            self.dataset_size = len(self.db)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.sampling_size is not None:\n",
        "            return {'input_ids': self.sampled_db[idx]['input_ids'],\n",
        "                    'attention_mask': self.sampled_db[idx]['attention_mask'],\n",
        "                    'labels': self.sampled_db[idx]['input_ids'].copy()}\n",
        "        else:\n",
        "            return {'input_ids': self.db[idx]['input_ids'],\n",
        "                    'attention_mask': self.db[idx]['attention_mask'],\n",
        "                    'labels': self.db[idx]['input_ids'].copy()}"
      ],
      "metadata": {
        "id": "pk90h5aDx2q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a callback to update the datasets and save a checkpoint of the best epoch yet."
      ],
      "metadata": {
        "id": "8sJYGLVq3sJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainerCallback(TrainerCallback):\n",
        "\n",
        "    def __init__(self, best_validation_yet=99999, model=None, train_dataset=None, eval_dataset=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.best_validation_loss = best_validation_yet\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.eval_dataset = eval_dataset\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, train_dataloader=None, eval_dataloader=None, **kwargs):\n",
        "        self.train_dataset.update_dataset()\n",
        "        self.eval_dataset.update_dataset()\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
        "        # print(metrics.keys())\n",
        "\n",
        "        try:\n",
        "            perplexity = np.exp(metrics[\"eval_loss\"])\n",
        "        except OverflowError:\n",
        "            perplexity = float(\"inf\")\n",
        "\n",
        "        metrics['perplexity'] = perplexity\n",
        "\n",
        "        # print(\"perplexity={}\".format(metrics['perplexity']))\n",
        "\n",
        "        if metrics['eval_loss'] < self.best_validation_loss:\n",
        "            self.model.save_pretrained(os.path.join(TRAIN_OUTPUT_FOLDER, \n",
        "                                                    \"checkpoint-{}-{:.4f}\".format(state.global_step,\n",
        "                                                                                  metrics['eval_loss'])))\n",
        "            self.best_validation_loss = metrics['eval_loss']"
      ],
      "metadata": {
        "id": "4zE1D83l2vYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data"
      ],
      "metadata": {
        "id": "WpuFpHXyN-cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(data_blocks[:-1], sampling_size=5000)"
      ],
      "metadata": {
        "id": "ZJc4Ak_H5tw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d261a1e4-c301-4c57-8ac1-cd4a36499477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = Dataset(data_blocks, len(data_blocks) - 1, sampling_size=3000)"
      ],
      "metadata": {
        "id": "Dp8cjYTZ-VJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a284ce1-f3a3-4e23-a184-9530284775e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 24; change_file_index=False...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model"
      ],
      "metadata": {
        "id": "Awzi5Ads5J6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "print('Parameters', model.num_parameters())"
      ],
      "metadata": {
        "id": "g1QZgeFy58CM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9c87ac-c29f-4470-b01d-ffa190fce4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters 125239296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the trainer"
      ],
      "metadata": {
        "id": "PrOgu5k96LTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=12\n",
        "epochs=200"
      ],
      "metadata": {
        "id": "kT5rUueI658D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                    num_train_epochs=epochs,\n",
        "                                    per_device_train_batch_size=batch_size,\n",
        "                                    per_device_eval_batch_size=batch_size,\n",
        "                                    evaluation_strategy='epoch',\n",
        "                                    save_strategy='no',\n",
        "                                    logging_strategy='steps',\n",
        "                                    logging_steps=10,\n",
        "                                    save_total_limit=10,\n",
        "                                    # report_to='comet_ml',\n",
        "                                    dataloader_num_workers=4,\n",
        "                                    dataloader_pin_memory=False,\n",
        "                                    fp16=True)"
      ],
      "metadata": {
        "id": "99L0Mq_p7s4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = CustomTrainerCallback(best_validation_yet=1.946620, \n",
        "                                         model=model, \n",
        "                                         train_dataset=train_dataset, \n",
        "                                         eval_dataset=eval_dataset)"
      ],
      "metadata": {
        "id": "phSscTZI3hO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = epochs * len(train_dataset)\n",
        "\n",
        "optimzer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-3)\n",
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimzer, \n",
        "                                                               num_training_steps * 0.1, \n",
        "                                                               num_training_steps, \n",
        "                                                               num_cycles=40)"
      ],
      "metadata": {
        "id": "8y9rWOcRbZoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  args=training_params,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=eval_dataset,\n",
        "                  callbacks=[trainer_callback],\n",
        "                  optimizers=(optimzer, scheduler)\n",
        "                 )"
      ],
      "metadata": {
        "id": "77Sgz4OB8nMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train(resume_from_checkpoint=\"trained_model/checkpoint-5000-1.9466\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTFtBSwwgKdu",
        "outputId": "6527fc96-45bd-4320-82a5-9d4f02d2c1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28256' max='83400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28256/83400 5:59:54 < 11:42:26, 1.31 it/s, Epoch 67.76/200]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.906800</td>\n",
              "      <td>2.001901</td>\n",
              "      <td>7.403119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.945800</td>\n",
              "      <td>1.983841</td>\n",
              "      <td>7.270619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.927600</td>\n",
              "      <td>2.000927</td>\n",
              "      <td>7.395912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.041400</td>\n",
              "      <td>1.992583</td>\n",
              "      <td>7.334456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.927400</td>\n",
              "      <td>1.993441</td>\n",
              "      <td>7.340751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.960900</td>\n",
              "      <td>1.988364</td>\n",
              "      <td>7.303577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.909200</td>\n",
              "      <td>1.980761</td>\n",
              "      <td>7.248260</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>2.004877</td>\n",
              "      <td>7.425181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.000800</td>\n",
              "      <td>1.990216</td>\n",
              "      <td>7.317112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.987100</td>\n",
              "      <td>1.993745</td>\n",
              "      <td>7.342981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.087500</td>\n",
              "      <td>1.983501</td>\n",
              "      <td>7.268147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.858800</td>\n",
              "      <td>2.004573</td>\n",
              "      <td>7.422921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.931000</td>\n",
              "      <td>1.981514</td>\n",
              "      <td>7.253716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.948500</td>\n",
              "      <td>1.970720</td>\n",
              "      <td>7.175839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.045500</td>\n",
              "      <td>2.009354</td>\n",
              "      <td>7.458498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.990400</td>\n",
              "      <td>2.005013</td>\n",
              "      <td>7.426190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.038300</td>\n",
              "      <td>1.986651</td>\n",
              "      <td>7.291075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.024400</td>\n",
              "      <td>1.995216</td>\n",
              "      <td>7.353790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.072800</td>\n",
              "      <td>1.989839</td>\n",
              "      <td>7.314355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.973300</td>\n",
              "      <td>1.964579</td>\n",
              "      <td>7.131907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.051100</td>\n",
              "      <td>1.991660</td>\n",
              "      <td>7.327688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.011500</td>\n",
              "      <td>1.988304</td>\n",
              "      <td>7.303135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.937200</td>\n",
              "      <td>1.990043</td>\n",
              "      <td>7.315850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.089500</td>\n",
              "      <td>1.999372</td>\n",
              "      <td>7.384418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.003600</td>\n",
              "      <td>1.981304</td>\n",
              "      <td>7.252191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.946800</td>\n",
              "      <td>1.976771</td>\n",
              "      <td>7.219397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.004200</td>\n",
              "      <td>2.016471</td>\n",
              "      <td>7.511772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.912500</td>\n",
              "      <td>1.997196</td>\n",
              "      <td>7.368366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.959500</td>\n",
              "      <td>1.986515</td>\n",
              "      <td>7.290082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.906400</td>\n",
              "      <td>1.995396</td>\n",
              "      <td>7.355119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.016600</td>\n",
              "      <td>1.989580</td>\n",
              "      <td>7.312462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.986200</td>\n",
              "      <td>1.991290</td>\n",
              "      <td>7.324979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.948800</td>\n",
              "      <td>1.998203</td>\n",
              "      <td>7.375791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.960600</td>\n",
              "      <td>1.989055</td>\n",
              "      <td>7.308624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.078100</td>\n",
              "      <td>1.987846</td>\n",
              "      <td>7.299797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.009700</td>\n",
              "      <td>2.003326</td>\n",
              "      <td>7.413676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.932700</td>\n",
              "      <td>1.986378</td>\n",
              "      <td>7.289085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.866600</td>\n",
              "      <td>1.989840</td>\n",
              "      <td>7.314361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.998000</td>\n",
              "      <td>2.001736</td>\n",
              "      <td>7.401898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.009100</td>\n",
              "      <td>2.008030</td>\n",
              "      <td>7.448629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.039700</td>\n",
              "      <td>1.990573</td>\n",
              "      <td>7.319726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.964300</td>\n",
              "      <td>1.992836</td>\n",
              "      <td>7.336312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.992700</td>\n",
              "      <td>1.996681</td>\n",
              "      <td>7.364570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.956400</td>\n",
              "      <td>1.982139</td>\n",
              "      <td>7.258254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.935000</td>\n",
              "      <td>1.992881</td>\n",
              "      <td>7.336639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.968000</td>\n",
              "      <td>2.003645</td>\n",
              "      <td>7.416038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.040800</td>\n",
              "      <td>1.989620</td>\n",
              "      <td>7.312755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.038000</td>\n",
              "      <td>1.989157</td>\n",
              "      <td>7.309370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.040900</td>\n",
              "      <td>1.979891</td>\n",
              "      <td>7.241951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.920500</td>\n",
              "      <td>1.982180</td>\n",
              "      <td>7.258547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.948600</td>\n",
              "      <td>1.977785</td>\n",
              "      <td>7.226715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.810000</td>\n",
              "      <td>2.004402</td>\n",
              "      <td>7.421654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.984200</td>\n",
              "      <td>1.990976</td>\n",
              "      <td>7.322674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.964300</td>\n",
              "      <td>1.992307</td>\n",
              "      <td>7.332430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.966100</td>\n",
              "      <td>1.993956</td>\n",
              "      <td>7.344529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.975800</td>\n",
              "      <td>1.980850</td>\n",
              "      <td>7.248905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.929500</td>\n",
              "      <td>1.993192</td>\n",
              "      <td>7.338923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.926100</td>\n",
              "      <td>1.962231</td>\n",
              "      <td>7.115184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.001600</td>\n",
              "      <td>1.982933</td>\n",
              "      <td>7.264019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.951600</td>\n",
              "      <td>1.991927</td>\n",
              "      <td>7.329647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.862000</td>\n",
              "      <td>1.973956</td>\n",
              "      <td>7.199101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.994700</td>\n",
              "      <td>1.996144</td>\n",
              "      <td>7.360616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.925100</td>\n",
              "      <td>1.983196</td>\n",
              "      <td>7.265928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.980800</td>\n",
              "      <td>1.986984</td>\n",
              "      <td>7.293502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.003800</td>\n",
              "      <td>1.981477</td>\n",
              "      <td>7.253445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.939900</td>\n",
              "      <td>1.977873</td>\n",
              "      <td>7.227356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.042400</td>\n",
              "      <td>1.958298</td>\n",
              "      <td>7.087256</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 20; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 21; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 22; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 23; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 20; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 21; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 22; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 23; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 5000\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CX-nHhLpc23F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}