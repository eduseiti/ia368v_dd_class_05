{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNfJxjaNtlo2HTS7Z7QzWRu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_05/blob/main/LM_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Causal Language Model (CLM) fine-tuning\n",
        "\n",
        "This notebook executes the fine-tuning of **facebook/opt-125m** model, over the mc4 pt dataset samples prepared by the `LM_training_dataset_preparation.ipynb` notebook."
      ],
      "metadata": {
        "id": "jm10mcBmMyUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "IyoODiJf7UDV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_05\"\n",
        "\n",
        "API_KEYS_FILE=\"/content/drive/MyDrive/unicamp/ia368v_dd/api_keys_20230324.json\"\n",
        "\n",
        "TRAIN_OUTPUT_FOLDER=\"./trained_model\"\n",
        "\n",
        "NORMALIZED_DATA_BLOCKS_PARTIAL_FILENAME=\"normalized_samples_block_*\""
      ],
      "metadata": {
        "id": "gB0nBfaiLYJC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import json"
      ],
      "metadata": {
        "id": "J1H5lQXWbFF5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "mFNsI1WXLtsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72dcae44-4209-4daa-8002-973fdb811c66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(API_KEYS_FILE) as inputFile:\n",
        "#     api_keys = json.load(inputFile)\n",
        "\n",
        "# os.environ[\"COMET_API_KEY\"] = api_keys['comet_ml']\n",
        "# os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "# os.environ['COMET_MODE'] = \"ONLINE\""
      ],
      "metadata": {
        "id": "Exz2AKVDbGmK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from comet_ml import Experiment\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, default_data_collator, TrainerCallback\n",
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union"
      ],
      "metadata": {
        "id": "2iEim9qqLRab"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "DcXd955oJhRE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "Iet6vPCwJhRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bc5719-bf73-4970-8b91-122b45df37d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "44pKblR9uKJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26fc3afb-9a14-4a7f-9a4a-b05cf5bfdeb4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 26 00:49:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   57C    P8    11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME=\"facebook/opt-125m\""
      ],
      "metadata": {
        "id": "7k9UgWy7Wf77"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link to Comet ML reporting"
      ],
      "metadata": {
        "id": "c2pyJsxgHxCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment(api_key=api_keys['comet_ml'], \n",
        "#            project_name=\"causal-language-model-fine-tuning\",\n",
        "#            workspace=\"eduseiti\")"
      ],
      "metadata": {
        "id": "Rxf3Y8TjH0iB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the list of normalized-tokenized samples data blocked_samples\n",
        "\n",
        "The mc4 pt dataset sample has already been tokenized and size-normalized to 512, which is the model input size.\n",
        "\n",
        "Each data block contains a list of prepared samples, each of which can be directly fed to the model:\n",
        "\n",
        "```\n",
        "    {'input_ids': <list-of-512-tokens>,\n",
        "     'attention_masks': <list-of-512-attention-masks>}\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GLsQX2lUwJtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_blocks = glob.glob(NORMALIZED_DATA_BLOCKS_PARTIAL_FILENAME)"
      ],
      "metadata": {
        "id": "eMIwdJE6wJyU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_blocks"
      ],
      "metadata": {
        "id": "_QGhlPPRwJ2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52338d01-4105-4ad5-8ccc-ff3e56ae6d5c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['normalized_samples_block_00.pkl',\n",
              " 'normalized_samples_block_01.pkl',\n",
              " 'normalized_samples_block_02.pkl',\n",
              " 'normalized_samples_block_03.pkl',\n",
              " 'normalized_samples_block_04.pkl',\n",
              " 'normalized_samples_block_05.pkl',\n",
              " 'normalized_samples_block_06.pkl',\n",
              " 'normalized_samples_block_07.pkl',\n",
              " 'normalized_samples_block_08.pkl',\n",
              " 'normalized_samples_block_09.pkl',\n",
              " 'normalized_samples_block_10.pkl',\n",
              " 'normalized_samples_block_11.pkl',\n",
              " 'normalized_samples_block_12.pkl',\n",
              " 'normalized_samples_block_13.pkl',\n",
              " 'normalized_samples_block_14.pkl',\n",
              " 'normalized_samples_block_15.pkl',\n",
              " 'normalized_samples_block_16.pkl',\n",
              " 'normalized_samples_block_17.pkl',\n",
              " 'normalized_samples_block_18.pkl',\n",
              " 'normalized_samples_block_19.pkl',\n",
              " 'normalized_samples_block_20.pkl',\n",
              " 'normalized_samples_block_21.pkl',\n",
              " 'normalized_samples_block_22.pkl',\n",
              " 'normalized_samples_block_23.pkl',\n",
              " 'normalized_samples_block_24.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the dataset class"
      ],
      "metadata": {
        "id": "SkHAtH-wMNV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, samples_blocks_filenames, fixed_data_block_index=None, sampling_size=None):\n",
        "        self.samples_blocks_filenames = samples_blocks_filenames\n",
        "\n",
        "        if fixed_data_block_index is not None:\n",
        "            self.current_file_index = fixed_data_block_index\n",
        "            self.change_file_index = False\n",
        "        else:\n",
        "            self.current_file_index = 0\n",
        "            self.change_file_index = True\n",
        "\n",
        "        with open(samples_blocks_filenames[self.current_file_index], \"rb\") as inputFile:\n",
        "            self.db = pickle.load(inputFile)\n",
        "\n",
        "        print(\"Dataset loading samples block {}; change_file_index={}...\".format(self.current_file_index, self.change_file_index))\n",
        "\n",
        "        self.sampling_size = sampling_size\n",
        "    \n",
        "        if self.sampling_size is not None:\n",
        "            self.dataset_size = self.sampling_size\n",
        "            self.sample_data()\n",
        "        else:\n",
        "            self.dataset_size = len(self.db)\n",
        "\n",
        "\n",
        "\n",
        "    def sample_data(self):\n",
        "        self.selected_samples = np.random.choice(list(range(len(self.db))), self.sampling_size, replace=False)\n",
        "        self.sampled_db = [self.db[i] for i in self.selected_samples]\n",
        "\n",
        "        print(\"Updating the sampled dataset itens; sample DB size: {}\".format(len(self.sampled_db)))\n",
        "\n",
        "\n",
        "\n",
        "    def update_dataset(self):\n",
        "        if self.change_file_index:\n",
        "            self.current_file_index = (self.current_file_index + 1) % len(self.samples_blocks_filenames)\n",
        "\n",
        "            with open(self.samples_blocks_filenames[self.current_file_index], \"rb\") as inputFile:\n",
        "                self.db = pickle.load(inputFile)\n",
        "\n",
        "            print(\"Updating dataset loading samples block {}; change_file_index={}...\".format(self.current_file_index, self.change_file_index))\n",
        "\n",
        "        if self.sampling_size is not None:\n",
        "            self.sample_data()\n",
        "        else:\n",
        "            self.dataset_size = len(self.db)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if self.sampling_size is not None:\n",
        "            return {'input_ids': self.sampled_db[idx]['input_ids'],\n",
        "                    'attention_mask': self.sampled_db[idx]['attention_mask'],\n",
        "                    'labels': self.sampled_db[idx]['input_ids'].copy()}\n",
        "        else:\n",
        "            return {'input_ids': self.db[idx]['input_ids'],\n",
        "                    'attention_mask': self.db[idx]['attention_mask'],\n",
        "                    'labels': self.db[idx]['input_ids'].copy()}"
      ],
      "metadata": {
        "id": "pk90h5aDx2q2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a callback to update the datasets"
      ],
      "metadata": {
        "id": "8sJYGLVq3sJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetUpdaterCallback(TrainerCallback):\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, **kwargs):\n",
        "        train_dataset.update_dataset()\n",
        "        eval_dataset.update_dataset()"
      ],
      "metadata": {
        "id": "4zE1D83l2vYN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the data"
      ],
      "metadata": {
        "id": "WpuFpHXyN-cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(data_blocks[:-1], sampling_size=3000)"
      ],
      "metadata": {
        "id": "ZJc4Ak_H5tw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66616979-f46d-427f-e3b3-61c3fa795163"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = Dataset(data_blocks, len(data_blocks) - 1, sampling_size=1000)"
      ],
      "metadata": {
        "id": "Dp8cjYTZ-VJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23de7062-f419-40fe-9973-99c6696c700f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loading samples block 24; change_file_index=False...\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the model"
      ],
      "metadata": {
        "id": "Awzi5Ads5J6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "print('Parameters', model.num_parameters())"
      ],
      "metadata": {
        "id": "g1QZgeFy58CM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45b2d4e-1716-4a0b-e4b6-58a348ee5890"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters 125239296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the trainer"
      ],
      "metadata": {
        "id": "PrOgu5k96LTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=12"
      ],
      "metadata": {
        "id": "kT5rUueI658D"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                    num_train_epochs=100,\n",
        "                                    per_device_train_batch_size=batch_size,\n",
        "                                    per_device_eval_batch_size=batch_size,\n",
        "                                    evaluation_strategy='epoch',\n",
        "                                    save_strategy='epoch',\n",
        "                                    logging_strategy='steps',\n",
        "                                    logging_steps=10,\n",
        "                                    save_total_limit=10,\n",
        "                                    # report_to='comet_ml',\n",
        "                                    learning_rate=2e-4,\n",
        "                                    weight_decay=1e-2,\n",
        "                                    dataloader_num_workers=4,\n",
        "                                    dataloader_pin_memory=False,\n",
        "                                    optim='adamw_torch',\n",
        "                                    fp16=True)"
      ],
      "metadata": {
        "id": "99L0Mq_p7s4M"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = DatasetUpdaterCallback()"
      ],
      "metadata": {
        "id": "phSscTZI3hO4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model,\n",
        "                     args=training_params,\n",
        "                     train_dataset=train_dataset,\n",
        "                     eval_dataset=eval_dataset,\n",
        "                     callbacks=[trainer_callback]\n",
        "                     )"
      ],
      "metadata": {
        "id": "77Sgz4OB8nMp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTFtBSwwgKdu",
        "outputId": "fda553a0-ad1f-4f53-ae33-75ce5712e2f4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25000/25000 5:06:39, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.872200</td>\n",
              "      <td>2.790410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.740700</td>\n",
              "      <td>2.707587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.622000</td>\n",
              "      <td>2.666003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.564700</td>\n",
              "      <td>2.577554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.646600</td>\n",
              "      <td>2.534735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.617100</td>\n",
              "      <td>2.499710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.516200</td>\n",
              "      <td>2.489207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.507700</td>\n",
              "      <td>2.482321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.387100</td>\n",
              "      <td>2.460213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.486100</td>\n",
              "      <td>2.411253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.526700</td>\n",
              "      <td>2.415589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.354400</td>\n",
              "      <td>2.412475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.483300</td>\n",
              "      <td>2.379616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.419700</td>\n",
              "      <td>2.330498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.464700</td>\n",
              "      <td>2.369917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.413700</td>\n",
              "      <td>2.407908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.422100</td>\n",
              "      <td>2.324724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.370800</td>\n",
              "      <td>2.338324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.214500</td>\n",
              "      <td>2.323711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.426700</td>\n",
              "      <td>2.266692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.332200</td>\n",
              "      <td>2.329203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.379000</td>\n",
              "      <td>2.309469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.379200</td>\n",
              "      <td>2.276808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.346000</td>\n",
              "      <td>2.288520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.322000</td>\n",
              "      <td>2.270638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.286900</td>\n",
              "      <td>2.244335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>2.297000</td>\n",
              "      <td>2.297158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.185500</td>\n",
              "      <td>2.267094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.228000</td>\n",
              "      <td>2.268818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.332600</td>\n",
              "      <td>2.239105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.221800</td>\n",
              "      <td>2.226401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.259900</td>\n",
              "      <td>2.254891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.225800</td>\n",
              "      <td>2.262799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.361000</td>\n",
              "      <td>2.241332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.180900</td>\n",
              "      <td>2.235283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.314000</td>\n",
              "      <td>2.220604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.208500</td>\n",
              "      <td>2.210865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.114900</td>\n",
              "      <td>2.220377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.271300</td>\n",
              "      <td>2.185976</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.235600</td>\n",
              "      <td>2.203533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.352400</td>\n",
              "      <td>2.196849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.135500</td>\n",
              "      <td>2.205533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.259400</td>\n",
              "      <td>2.210718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>2.139000</td>\n",
              "      <td>2.177302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.214600</td>\n",
              "      <td>2.165892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>2.102700</td>\n",
              "      <td>2.191660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>2.131700</td>\n",
              "      <td>2.158696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>2.142800</td>\n",
              "      <td>2.154267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.216100</td>\n",
              "      <td>2.144879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.166800</td>\n",
              "      <td>2.137830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>2.195300</td>\n",
              "      <td>2.128077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.160500</td>\n",
              "      <td>2.202070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.099800</td>\n",
              "      <td>2.156335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.198600</td>\n",
              "      <td>2.174073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.106700</td>\n",
              "      <td>2.159763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.227400</td>\n",
              "      <td>2.125372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.066600</td>\n",
              "      <td>2.136156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.198800</td>\n",
              "      <td>2.096716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>2.230900</td>\n",
              "      <td>2.127225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.153800</td>\n",
              "      <td>2.129169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.138800</td>\n",
              "      <td>2.101413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.150100</td>\n",
              "      <td>2.110209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.155900</td>\n",
              "      <td>2.132833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.158400</td>\n",
              "      <td>2.145720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.217800</td>\n",
              "      <td>2.090046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.192100</td>\n",
              "      <td>2.112899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.121000</td>\n",
              "      <td>2.063437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.142500</td>\n",
              "      <td>2.102070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.230200</td>\n",
              "      <td>2.104604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.117400</td>\n",
              "      <td>2.058729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.124500</td>\n",
              "      <td>2.115729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.138500</td>\n",
              "      <td>2.114519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.125800</td>\n",
              "      <td>2.063637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.048200</td>\n",
              "      <td>2.119872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.085400</td>\n",
              "      <td>2.096557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.119800</td>\n",
              "      <td>2.128890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.059400</td>\n",
              "      <td>2.077878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.012400</td>\n",
              "      <td>2.108656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.184200</td>\n",
              "      <td>2.081046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.163000</td>\n",
              "      <td>2.104289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.039500</td>\n",
              "      <td>2.078139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.087500</td>\n",
              "      <td>2.087730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.102300</td>\n",
              "      <td>2.091651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>2.029900</td>\n",
              "      <td>2.041834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.135400</td>\n",
              "      <td>2.075941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>2.071700</td>\n",
              "      <td>2.031648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>2.040000</td>\n",
              "      <td>2.067657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>2.076900</td>\n",
              "      <td>2.076730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.991800</td>\n",
              "      <td>2.034089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.156800</td>\n",
              "      <td>2.070031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>2.104800</td>\n",
              "      <td>2.108285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>2.083600</td>\n",
              "      <td>2.046177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.998600</td>\n",
              "      <td>2.036034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.986400</td>\n",
              "      <td>2.027431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.138900</td>\n",
              "      <td>2.038956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.981000</td>\n",
              "      <td>2.030885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.161400</td>\n",
              "      <td>2.036836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>2.024800</td>\n",
              "      <td>2.043764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>2.086600</td>\n",
              "      <td>2.031417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.072200</td>\n",
              "      <td>2.047127</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 20; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 21; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 22; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 23; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 20; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 21; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 22; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 23; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 20; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 21; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 22; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 23; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 5; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 6; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 7; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 8; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 9; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 10; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 11; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 12; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 13; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 14; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 15; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 16; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 17; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 18; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 19; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 20; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 21; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 22; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 23; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 0; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 1; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 2; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 3; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n",
            "Updating dataset loading samples block 4; change_file_index=True...\n",
            "Updating the sampled dataset itens; sample DB size: 3000\n",
            "Updating the sampled dataset itens; sample DB size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "PgwWpgyIgaO6"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}